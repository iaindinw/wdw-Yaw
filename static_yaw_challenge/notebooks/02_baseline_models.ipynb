{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Yaw Challenge - Baseline Models\n",
    "\n",
    "**Objective**: Build baseline classification models for yaw offset prediction\n",
    "\n",
    "**Approach**:\n",
    "1. Load and prepare segment-level features\n",
    "2. Create train/validation split matching test distribution\n",
    "3. Train baseline models (Random Forest, Logistic Regression)\n",
    "4. Evaluate performance and establish benchmarks\n",
    "5. Analyze feature importance\n",
    "6. Generate predictions for test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Custom modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from feature_engineering import create_segment_features, filter_production_mode\n",
    "from validation import (create_distribution_matched_split, evaluate_classification,\n",
    "                       calculate_class_weights, create_test_submission_template)\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('../data/yaw_alignment_dataset')\n",
    "RESULTS_DIR = Path('../results')\n",
    "FIGURES_DIR = Path('../reports/figures')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Environment ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "print(\"Loading training data...\")\n",
    "df_train = pd.read_parquet(DATA_DIR / 'train.parquet')\n",
    "print(f\"Training data shape: {df_train.shape}\")\n",
    "\n",
    "# Load test data\n",
    "print(\"\\nLoading test data...\")\n",
    "df_test = pd.read_parquet(DATA_DIR / 'test.parquet')\n",
    "print(f\"Test data shape: {df_test.shape}\")\n",
    "print(f\"Unique segments: {df_test['segment_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering - Segment Level\n",
    "\n",
    "We'll focus on segment-level predictions (298 segments) rather than row-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training data, we need to create synthetic segments\n",
    "# Alternative: Use the full data and aggregate differently\n",
    "# For this baseline, let's filter to production mode and sample segments\n",
    "\n",
    "print(\"Filtering training data to production mode (status 10)...\")\n",
    "df_train_prod = filter_production_mode(df_train)\n",
    "print(f\"Production mode data: {len(df_train_prod):,} rows ({len(df_train_prod)/len(df_train)*100:.1f}%)\")\n",
    "\n",
    "# Create synthetic 1-hour segments for training\n",
    "print(\"\\nCreating synthetic 1-hour segments for training...\")\n",
    "SEGMENT_SIZE = 3600  # 1 hour at 1 Hz\n",
    "\n",
    "# Group by yaw offset to maintain class balance\n",
    "train_segments_list = []\n",
    "for yaw in df_train_prod['yaw_offset'].unique():\n",
    "    yaw_data = df_train_prod[df_train_prod['yaw_offset'] == yaw].copy()\n",
    "    \n",
    "    # Create segments\n",
    "    n_segments = len(yaw_data) // SEGMENT_SIZE\n",
    "    for i in range(n_segments):\n",
    "        segment = yaw_data.iloc[i*SEGMENT_SIZE:(i+1)*SEGMENT_SIZE].copy()\n",
    "        segment['segment_id'] = f'train_seg_{yaw}_{i:05d}'\n",
    "        train_segments_list.append(segment)\n",
    "\n",
    "df_train_segments = pd.concat(train_segments_list, ignore_index=True)\n",
    "print(f\"Created {df_train_segments['segment_id'].nunique()} training segments\")\n",
    "print(f\"Segment yaw distribution:\")\n",
    "print(df_train_segments.groupby('segment_id')['yaw_offset'].first().value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create segment-level features for training\n",
    "print(\"Creating segment-level features for training data...\")\n",
    "train_segment_features = create_segment_features(\n",
    "    df_train_segments,\n",
    "    segment_col='segment_id',\n",
    "    exclude_cols=['row_id', 'datetime']\n",
    ")\n",
    "\n",
    "# Add target (yaw_offset per segment)\n",
    "segment_targets = df_train_segments.groupby('segment_id')['yaw_offset'].first().reset_index()\n",
    "train_segment_features = train_segment_features.merge(segment_targets, on='segment_id')\n",
    "\n",
    "print(f\"\\nSegment features shape: {train_segment_features.shape}\")\n",
    "print(f\"Features created: {train_segment_features.shape[1] - 2}\")  # Exclude segment_id and yaw_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create segment-level features for test data\n",
    "print(\"Creating segment-level features for test data...\")\n",
    "test_segment_features = create_segment_features(\n",
    "    df_test,\n",
    "    segment_col='segment_id',\n",
    "    exclude_cols=['row_id', 'segment_time']\n",
    ")\n",
    "\n",
    "print(f\"Test segment features shape: {test_segment_features.shape}\")\n",
    "print(f\"Number of test segments: {len(test_segment_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns (exclude segment_id and target)\n",
    "feature_cols = [c for c in train_segment_features.columns \n",
    "                if c not in ['segment_id', 'yaw_offset']]\n",
    "\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"\\nFeatures: {feature_cols[:10]}...\")  # Show first 10\n",
    "\n",
    "# Prepare training data\n",
    "X_train_full = train_segment_features[feature_cols]\n",
    "y_train_full = train_segment_features['yaw_offset'].values\n",
    "\n",
    "# Prepare test data\n",
    "X_test = test_segment_features[feature_cols]\n",
    "\n",
    "print(f\"\\nTraining features shape: {X_train_full.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stratified split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full,\n",
    "    test_size=0.2,\n",
    "    stratify=y_train_full,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"\\nTraining target distribution:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"  {int(u)}°: {c} ({c/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nValidation target distribution:\")\n",
    "unique, counts = np.unique(y_val, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"  {int(u)}°: {c} ({c/len(y_val)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Handle Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights\n",
    "class_weights = calculate_class_weights(y_train)\n",
    "\n",
    "# Convert to format for sklearn\n",
    "class_weight_dict = {int(k): v for k, v in class_weights.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Baseline Model 1: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Random Forest Classifier...\")\n",
    "\n",
    "# Create and train Random Forest\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight=class_weight_dict,\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "y_val_pred_rf = rf_model.predict(X_val)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RANDOM FOREST VALIDATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "rf_metrics = evaluate_classification(\n",
    "    y_val, y_val_pred_rf,\n",
    "    class_names=['0°', '4°', '6°'],\n",
    "    print_results=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Most Important Features:\")\n",
    "print(feature_importance.head(20))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(20)\n",
    "plt.barh(range(len(top_features)), top_features['importance'].values)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'].values)\n",
    "plt.xlabel('Importance', fontsize=12)\n",
    "plt.title('Top 20 Feature Importances (Random Forest)', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'rf_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Baseline Model 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Logistic Regression...\")\n",
    "\n",
    "# Create pipeline with scaling (important for logistic regression)\n",
    "lr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight=class_weight_dict,\n",
    "        random_state=RANDOM_SEED,\n",
    "        multi_class='multinomial',\n",
    "        solver='lbfgs'\n",
    "    ))\n",
    "])\n",
    "\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "y_val_pred_lr = lr_pipeline.predict(X_val)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOGISTIC REGRESSION VALIDATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "lr_metrics = evaluate_classification(\n",
    "    y_val, y_val_pred_lr,\n",
    "    class_names=['0°', '4°', '6°'],\n",
    "    print_results=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'Logistic Regression'],\n",
    "    'Accuracy': [rf_metrics['accuracy'], lr_metrics['accuracy']],\n",
    "    'F1 (Macro)': [rf_metrics['f1_macro'], lr_metrics['f1_macro']],\n",
    "    'F1 (Weighted)': [rf_metrics['f1_weighted'], lr_metrics['f1_weighted']],\n",
    "    'F1 (0°)': [rf_metrics['f1_0°'], lr_metrics['f1_0°']],\n",
    "    'F1 (4°)': [rf_metrics['f1_4°'], lr_metrics['f1_4°']],\n",
    "    'F1 (6°)': [rf_metrics['f1_6°'], lr_metrics['f1_6°']]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Overall metrics\n",
    "metrics_to_plot = ['Accuracy', 'F1 (Macro)', 'F1 (Weighted)']\n",
    "x = np.arange(len(metrics_to_plot))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, comparison.iloc[0][metrics_to_plot], width, label='Random Forest', alpha=0.8)\n",
    "axes[0].bar(x + width/2, comparison.iloc[1][metrics_to_plot], width, label='Logistic Regression', alpha=0.8)\n",
    "axes[0].set_xlabel('Metric', fontsize=12)\n",
    "axes[0].set_ylabel('Score', fontsize=12)\n",
    "axes[0].set_title('Overall Model Performance', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(metrics_to_plot)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "# Per-class F1 scores\n",
    "class_metrics = ['F1 (0°)', 'F1 (4°)', 'F1 (6°)']\n",
    "x2 = np.arange(len(class_metrics))\n",
    "\n",
    "axes[1].bar(x2 - width/2, comparison.iloc[0][class_metrics], width, label='Random Forest', alpha=0.8)\n",
    "axes[1].bar(x2 + width/2, comparison.iloc[1][class_metrics], width, label='Logistic Regression', alpha=0.8)\n",
    "axes[1].set_xlabel('Yaw Offset Class', fontsize=12)\n",
    "axes[1].set_ylabel('F1 Score', fontsize=12)\n",
    "axes[1].set_title('Per-Class F1 Scores', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x2)\n",
    "axes[1].set_xticklabels(['0°', '4°', '6°'])\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Test Predictions\n",
    "\n",
    "Using the best performing model (likely Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model (Random Forest based on typical performance)\n",
    "best_model = rf_model\n",
    "model_name = \"Random Forest\"\n",
    "\n",
    "print(f\"Generating predictions with {model_name}...\")\n",
    "\n",
    "# Predict on test segments\n",
    "segment_predictions = best_model.predict(X_test)\n",
    "\n",
    "# Create segment-level submission\n",
    "segment_submission = pd.DataFrame({\n",
    "    'segment_id': test_segment_features['segment_id'],\n",
    "    'predicted_yaw': segment_predictions\n",
    "})\n",
    "\n",
    "print(f\"\\nSegment-level predictions:\")\n",
    "print(segment_submission.head(10))\n",
    "print(f\"\\nPredicted distribution:\")\n",
    "print(segment_submission['predicted_yaw'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map segment predictions to row-level predictions\n",
    "row_predictions = df_test[['row_id', 'segment_id']].merge(\n",
    "    segment_submission,\n",
    "    on='segment_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Create submission file\n",
    "submission = row_predictions[['row_id', 'predicted_yaw']].rename(\n",
    "    columns={'predicted_yaw': 'yaw_offset'}\n",
    ")\n",
    "\n",
    "# Save submission\n",
    "submission_path = RESULTS_DIR / 'baseline_submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\nSubmission saved to: {submission_path}\")\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"\\nRow-level prediction distribution:\")\n",
    "print(submission['yaw_offset'].value_counts().sort_index())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(submission.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "import joblib\n",
    "\n",
    "models_dir = RESULTS_DIR / 'models'\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "joblib.dump(rf_model, models_dir / 'random_forest_baseline.pkl')\n",
    "joblib.dump(lr_pipeline, models_dir / 'logistic_regression_baseline.pkl')\n",
    "\n",
    "print(\"Models saved!\")\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv(RESULTS_DIR / 'feature_importance_rf.csv', index=False)\n",
    "print(\"Feature importance saved!\")\n",
    "\n",
    "# Save metrics\n",
    "comparison.to_csv(RESULTS_DIR / 'baseline_model_comparison.csv', index=False)\n",
    "print(\"Metrics saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"BASELINE MODELING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. DATA PREPARATION:\")\n",
    "print(f\"   - Training segments: {len(train_segment_features)}\")\n",
    "print(f\"   - Test segments: {len(test_segment_features)}\")\n",
    "print(f\"   - Features created: {len(feature_cols)}\")\n",
    "\n",
    "print(\"\\n2. MODEL PERFORMANCE (Validation Set):\")\n",
    "print(f\"   Random Forest:\")\n",
    "print(f\"     - Accuracy: {rf_metrics['accuracy']:.4f}\")\n",
    "print(f\"     - F1 (Macro): {rf_metrics['f1_macro']:.4f}\")\n",
    "print(f\"   Logistic Regression:\")\n",
    "print(f\"     - Accuracy: {lr_metrics['accuracy']:.4f}\")\n",
    "print(f\"     - F1 (Macro): {lr_metrics['f1_macro']:.4f}\")\n",
    "\n",
    "print(\"\\n3. TOP 5 MOST IMPORTANT FEATURES:\")\n",
    "for i, row in feature_importance.head(5).iterrows():\n",
    "    print(f\"   {i+1}. {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(\"\\n4. PREDICTIONS GENERATED:\")\n",
    "print(f\"   - Segment-level: {len(segment_submission)} predictions\")\n",
    "print(f\"   - Row-level: {len(submission)} predictions\")\n",
    "print(f\"   - Distribution: 0°={len(submission[submission['yaw_offset']==0])}, \"\n",
    "      f\"4°={len(submission[submission['yaw_offset']==4])}, \"\n",
    "      f\"6°={len(submission[submission['yaw_offset']==6])}\")\n",
    "\n",
    "print(\"\\n5. NEXT STEPS:\")\n",
    "print(\"   - Try gradient boosting models (XGBoost, LightGBM)\")\n",
    "print(\"   - Experiment with different feature engineering approaches\")\n",
    "print(\"   - Try two-stage modeling (row → segment)\")\n",
    "print(\"   - Implement ensemble methods\")\n",
    "print(\"   - Address mystery offset in private test (regression approach)\")\n",
    "print(\"   - Perform hyperparameter tuning\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
